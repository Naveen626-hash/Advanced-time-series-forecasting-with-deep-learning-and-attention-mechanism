# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ULuAFW_pZl5W6ikCGKjTapQAfNETsnnn
"""



# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit

import streamlit as st
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import pickle
import numpy as _np
import os
import tensorflow as tf # Import tensorflow
from tensorflow.keras.layers import Attention
from tensorflow import keras # Import keras to access its saving utilities

# Define a custom AttentionWrapper to handle single input during deserialization
class AttentionWrapper(Attention):
    # Override the private _validate_inputs method to allow single tensor input
    def _validate_inputs(self, inputs, mask=None):
        # If the input is not a list, we will handle it in the call method,
        # so bypass the default validation error at this stage.
        if not isinstance(inputs, list):
            return
        else:
            # If it's already a list, let the base class perform its validation.
            # We call the original _validate_inputs from the parent class.
            super()._validate_inputs(inputs, mask)

    def call(self, inputs, training=None, mask=None):
        # The base Attention.call method will internally call self._validate_inputs first.
        # Our overridden _validate_inputs will now allow single inputs to pass without error.

        if not isinstance(inputs, list):
            # Assuming single input means query=value=inputs (self-attention)
            query = inputs
            value = inputs
            # Now, call the original Attention layer's call method with the correct list format.
            return super().call(inputs=[query, value], training=training, mask=mask)
        else:
            # If already a list, pass it as is
            return super().call(inputs=inputs, training=training, mask=mask)

    # Override compute_output_spec to handle single inputs during model loading/building
    def compute_output_spec(self, inputs, mask=None, return_attention_scores=False, training=None, use_causal_mask=False):
        if not isinstance(inputs, list):
            # For self-attention, duplicate the input for query and value
            inputs = [inputs, inputs]
        return super().compute_output_spec(inputs, mask, return_attention_scores, training, use_causal_mask)

# Explicitly add the AttentionWrapper to Keras's custom object registry for deserialization
# This makes Keras use our wrapper when it encounters an 'Attention' layer in the pickled model.
keras.utils.get_custom_objects().update({'Attention': AttentionWrapper})

st.set_page_config(layout="wide", page_title="LSTM+Attention Forecast")

MODEL_PATH = "/content/drive/MyDrive/model.pkl"
SCALER_PATH = "/content/drive/MyDrive/scaler.pkl"
DATA_PATH = "/content/drive/MyDrive/app/DAYTON_hourly.csv"

@st.cache_resource
def load_resources():
    # Load model using pickle
    with open(MODEL_PATH, 'rb') as f:
        model = pickle.load(f)
    # Load the scaler directly, assuming it's a MinMaxScaler object
    scaler = _np.load(SCALER_PATH, allow_pickle=True)
    return model, scaler

model, scaler = load_resources()

st.title('LSTM + Attention Forecast (Dayton hourly)')

# Load data
try:
    df = pd.read_csv(DATA_PATH)

    # Explicitly convert 'Datetime' to datetime objects and set as index
    df['Datetime'] = pd.to_datetime(df['Datetime'])
    df.set_index('Datetime', inplace=True)

    # Rename the 'DAYTON_MW' column to 'y' and ensure it's numeric
    df = df.rename(columns={'DAYTON_MW': 'y'})
    df['y'] = pd.to_numeric(df['y'], errors='coerce') # Coerce non-numeric values to NaN

    # Drop any rows where 'y' might have become NaN due to coercion
    df.dropna(subset=['y'], inplace=True)

    # Select only the 'y' column and resample
    df = df[['y']]
    df = df.resample('H').mean().ffill()
except Exception as e:
    st.error(f'Could not load dataset: {e}')
    st.stop()

st.subheader('Historical data')
st.line_chart(df['y'].rename('y'))

col1, col2 = st.columns([2,1])
with col2:
    input_len = st.number_input('Input window (hours)', min_value=6, max_value=72, value=24)
    horizon = st.number_input('Forecast horizon (hours)', min_value=1, max_value=48, value=6)
    run = st.button('Run forecast')

if run:
    # build last input_len hours
    last = df['y'].values[-input_len:]
    # scale
    scaled = scaler.transform(last.reshape(-1,1)).reshape(-1)
    X = scaled.reshape(1, input_len, 1)
    # If the pickled model is a Keras model, .predict() should work
    pred_scaled = model.predict(X)
    # take first horizon values
    pred = scaler.inverse_transform(pred_scaled.reshape(-1,1)).reshape(-1)
    # build index
    last_idx = df.index[-1]
    future_idx = pd.date_range(last_idx + pd.Timedelta(hours=1), periods=len(pred), freq='H')
    res = pd.Series(pred, index=future_idx, name='forecast')
    st.subheader('Forecast')
    st.line_chart(pd.concat([df['y'][-input_len:].rename('history'), res]))
    st.table(res.head(20))

st.markdown('**Notes:** This app loads a small LSTM+Attention model trained in the notebook. If you change `INPUT_LEN` or `HORIZON` in the notebook you should retrain or rebuild inputs appropriately.')